{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"check.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"xBiovuPwq9X2","colab_type":"code","colab":{}},"source":["import pandas as pd\n","df = pd.read_csv('ford_names.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D-yJLakqrMju","colab_type":"code","colab":{}},"source":["names = df['names'][0].split(',')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sYOYmFgesYM5","colab_type":"code","colab":{}},"source":["from bs4 import BeautifulSoup as bs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WTFoKsMIs8m2","colab_type":"code","colab":{}},"source":["import re\n","\n","TAG_RE = re.compile(r'<[^>]+>')\n","\n","def remove_tags(text):\n","    return TAG_RE.sub('', text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F-am5Rnyr6JN","colab_type":"code","colab":{}},"source":["cleaned_names = []"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zejlho6hrNns","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1597774169265,"user_tz":-330,"elapsed":2210,"user":{"displayName":"Akash Ravichandran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhfKFX11vGh6cZxd-XFGzg8KZ0eqt7nBXu2mgPT1Q=s64","userId":"05259929966764095227"}},"outputId":"76224d7b-9368-45bd-e4fb-557597267076"},"source":["from nltk.corpus import stopwords \n","from nltk.tokenize import word_tokenize \n","stop_words = ['[1]', '[2]', '[3]', '[4]', '[5]'] \n","\n","\n","for name in names:\n","  n = remove_tags(name)\n","  n = n.replace('Ford ', '')\n","  n = n.replace(' (Australia)', '')\n","  n = n.replace(' (North America)', '')\n","  n = n.replace(' (Europe)', '')\n","  n = n.replace(' (sixth generation)', '')\n","  n = n.replace(' (International)', '')\n","  n = n.replace(' (modified Thunderbird)', '')\n","  \n","  n = n.replace('/', '')\n","  if n not in stop_words:\n","    cleaned_names.append(n)\n","\n","  print(n)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Ka\n","Fiesta\n","Focus\n","Mondeo\n","Fiesta ST\n","Focus ST\n","Mustang\n","Ranger Raptor\n","F-150 Raptor\n","GT\n","S-Max\n","Galaxy\n","EcoSport\n","Puma\n","Escape\n","Kuga\n","Edge\n","Bronco\n","Bronco Sport\n","Mustang Mach-E\n","Explorer\n","Expedition\n","Transit Connect\n","E-Series\n","Transit Courier\n","Transit Custom\n","Transit\n","Ranger\n","F-150\n","Super Duty\n","Medium Duty Truck\n","Bronco\n","Capri\n","Corsair\n","Fairlane\n","Fairmont\n","Falcon\n","Futura\n","Maverick\n","Meteor\n","Landau\n","Laser\n","LTD\n","Telstar\n","Territory\n","2GA\n","300\n","7W\n","Abeille\n","Anglia\n","Aspire\n","B-Max\n","Capri\n","Capri\n","Comète\n","Consul\n","Consul Classic\n","Contour\n","Corcel\n","Corsair\n","Corsair\n","Cortina\n","Cougar\n","Country Sedan\n","Country Squire\n","Crestline\n","Crestliner\n","Crown Victoria\n","Custom\n","Custom Deluxe\n","Custom 500\n","Customline\n","CX\n","Delivery Car\n","Del Rey\n","Del Rio\n","Deluxe\n","Durango\n","Eifel\n","Eight\n","Elite\n","Escort\n","Escort\n","Escort\n","Escort\n","Executive\n","EXP\n","Fairlane\n","Fairmont\n","Falcon\n","Festiva\n","Fiesta\n","Figo\n","Five Hundred\n","Focus\n","Focus\n","Focus C-MAX\n","Freestyle\n","Fusion\n","Fusion\n","Futura\n","G6 & G6E\n","Galaxie\n","Granada\n","Granada\n","GT\n","GT40\n","GTX1\n","Ikon\n","Ka\n","Köln\n","Landau\n","Landau\n","Laser\n","LTD\n","LTD\n","LTD II\n","LTD Crown Victoria\n","Mainline\n","Mainline Coupe Utility\n","Maverick\n","Meteor\n","Model 4-46\n","Model 8-46\n","Model 01A02A\n","Model 11A1GA\n","Model 21A2GA\n","Model 18\n","Model 40\n","Model 48\n","Model 50\n","Model 67\n","Model 68\n","Model 69A6GA\n","Model 7377\n","Model 7478\n","Model 79A7GA\n","Model 81A\n","Model 82A\n","Model 87HA89A\n","Model 91\n","Model 92A\n","Model A\n","Model A\n","Model AC\n","Model B\n","Model B\n","Model C\n","Model C Ten\n","Model F\n","Model K\n","Model N\n","Model R\n","Model S\n","Model T\n","Model Y\n","Mondeo\n","Mondeo Metrostar\n","Mustang\n","Orion\n","Pampa\n","Parklane\n","Pilot\n","Pinto\n","Popular\n","Prefect\n","Probe\n","Pronto\n","Puma\n","Quadricycle\n","Ranchero\n","Ranch Wagon\n","Rheinland\n","Roadster\n","Royale\n","RS200\n","S-Max\n","Sedan Delivery\n","Scorpio\n","Sierra\n","Skyliner\n","Special\n","SportKa\n","Squire\n","Standard\n","StreetKa\n","Sunliner\n","Starliner\n","Super Deluxe\n","Taunus\n","Taurus\n","Telstar\n","Tempo\n","Ten-Ten\n","Thunderbird\n","Torino\n","Vedette\n","Vendome\n","Verona\n","Versailles\n","Victoria\n","XL\n","Zephyr\n","Zodiac\n","ZX2\n","C100\n","Capri RS\n","Escort RS 1700T\n","Escort RS Cosworth\n","Fiesta R5\n","Fiesta RS WRC\n","Focus RS WRC\n","G7\n","GT40\n","GT70\n","Mustang GTP\n","Mustang Maxum GTP\n","Mustang Probe\n","P68\n","P69\n","RS200\n","Sierra RS Cosworth\n","E-Series\n","Econoline\n","Husky\n","Spectron\n","Supervan\n","Thames 300E\n","Thames 400E\n","Tourneo\n","Tourneo Connect\n","Transit\n","Transit Connect\n","Transit Courier\n","Transit Custom\n","Transit Custom Nugget\n","C-MAX\n","S-Max\n","Galaxy\n","Windstar\n","Aerostar\n","Freestar\n","Bronco\n","Bronco II\n","Bronco\n","Ecosport\n","Edge\n","Endeavour\n","Escape\n","Everest\n","Excursion\n","Expedition\n","Explorer\n","Fiera\n","[1]\n","[2]\n","[3]\n","Flex\n","Freestyle\n","Fusion\n","Kuga\n","Raider\n","Taurus X\n","Territory\n","021C\n","4-Trac\n","Aerostar\n","Airstream\n","Atlas\n","B-Max\n","Bronco\n","Carousel\n","Comuta\n","Cougar 406\n","Desert Excursion\n","e.go\n","[4]\n","EcoSport\n","Evos\n","EX\n","Explorer America\n","Explorer Drifter\n","Explorer Sportsman\n","F-250 Super Chief\n","FAB1\n","Fairlane\n","Fiesta ST\n","Focus MA\n","Forty-Nine\n","FX-Atmos\n","HFX Aerostar\n","Granada Altair\n","GT\n","GT70\n","GT90\n","Gyron\n","Indigo\n","Interceptor\n","Iosis\n","Iosis X\n","Iosis MAX\n","Mach I Levacar\n","Magic Cruiser\n","Maya\n","Model U\n","Mustang\n","Mustang I\n","Mustang II\n","Mustang II Sportiva\n","Mustang Mach II\n","Mustang Mach III\n","Mustang Milano\n","[5]\n","Nucleon\n","Pockar\n","Powerforce\n","Probe I\n","Probe II\n","Probe III\n","Probe IV\n","Probe V\n","Prodigy\n","Ranger Force 5\n","Reflex\n","SAV\n","Saetta\n","Seattle-ite XXI\n","Shelby GR-1\n","Sport-Trac\n","Surf\n","Super Chief\n","SYNUS\n","TH!NK\n","Thunderbird\n","Thunderbird\n","Thunderbird\n","Transit Connect\n","Tridon\n","Turing Ka\n","Vertrek\n","Vega\n","Verve\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"G25psOO6tw9n","colab_type":"code","colab":{}},"source":["cleaned_names = set(cleaned_names)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qD3p_YjUbLRl","colab_type":"code","colab":{}},"source":["MyList = cleaned_names\n","MyFile=open('output.txt','w')\n","\n","for element in MyList:\n","     MyFile.write(element)\n","     MyFile.write('\\n')\n","MyFile.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nwm4l3jJ7X9E","colab_type":"code","colab":{}},"source":["import numpy as np\n","import random"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I5tU_UARWcHM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597774169271,"user_tz":-330,"elapsed":2185,"user":{"displayName":"Akash Ravichandran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhfKFX11vGh6cZxd-XFGzg8KZ0eqt7nBXu2mgPT1Q=s64","userId":"05259929966764095227"}},"outputId":"f04a25e3-7980-417c-a5f2-7431cb8cb2ed"},"source":["data = open('output.txt', 'r').read()\n","data = data.lower().strip()\n","chars = list(set(data))\n","data_size, vocab_size = len(data), len(chars)\n","print('There are %d total characters and %d unique characters in your data.' % (data_size, vocab_size))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["There are 2509 total characters and 43 unique characters in your data.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ATK8_1z5vKcc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1597774169271,"user_tz":-330,"elapsed":2175,"user":{"displayName":"Akash Ravichandran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhfKFX11vGh6cZxd-XFGzg8KZ0eqt7nBXu2mgPT1Q=s64","userId":"05259929966764095227"}},"outputId":"720c81c5-e43c-4538-c901-69180c5f84d8"},"source":["print(data)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["puma\n","taurus\n","cx\n","desert excursion\n","super deluxe\n","ecosport\n","2ga\n","seattle-ite xxi\n","thames 400e\n","mach i levacar\n","probe v\n","e-series\n","ltd\n","pampa\n","five hundred\n","model 01a02a\n","sport-trac\n","tridon\n","telstar\n","fiesta r5\n","streetka\n","gyron\n","super duty\n","mondeo metrostar\n","ex\n","model 79a7ga\n","model 92a\n","elite\n","escape\n","turing ka\n","explorer drifter\n","evos\n","custom\n","sierra rs cosworth\n","focus st\n","tempo\n","mustang i\n","focus\n","anglia\n","köln\n","transit courier\n","nucleon\n","explorer\n","transit connect\n","comuta\n","flex\n","mustang milano\n","consul classic\n","meteor\n","fab1\n","mustang ii\n","saetta\n","p69\n","corsair\n","ranger raptor\n","gtx1\n","model 8-46\n","sierra\n","probe iv\n","landau\n","del rio\n","powerforce\n","ranger\n","ten-ten\n","galaxy\n","contour\n","model f\n","surf\n","b-max\n","explorer sportsman\n","zephyr\n","del rey\n","gt90\n","starliner\n","medium duty truck\n","model 7478\n","eight\n","pinto\n","super chief\n","gt70\n","bronco sport\n","xl\n","model u\n","model b\n","granada altair\n","fusion\n","ltd ii\n","country sedan\n","model 67\n","roadster\n","model 81a\n","model 68\n","7w\n","durango\n","ranch wagon\n","escort rs 1700t\n","thunderbird\n","c-max\n","squire\n","magic cruiser\n","model n\n","model 40\n","fairmont\n","maya\n","model 48\n","mustang maxum gtp\n","sunliner\n","cougar 406\n","model ac\n","iosis x\n","f-150 raptor\n","escort\n","figo\n","granada\n","model 11a1ga\n","g7\n","probe iii\n","capri\n","escort rs cosworth\n","husky\n","probe\n","g6 & g6e\n","model c\n","reflex\n","021c\n","cortina\n","thames 300e\n","ecosport\n","popular\n","model r\n","endeavour\n","festiva\n","aspire\n","transit custom nugget\n","sportka\n","gt\n","prodigy\n","e.go\n","sav\n","taunus\n","consul\n","ltd crown victoria\n","sedan delivery\n","supervan\n","model 50\n","custom 500\n","mustang mach iii\n","mustang mach ii\n","ranger force 5\n","vertrek\n","galaxie\n","torino\n","mainline\n","pilot\n","model 87ha89a\n","mustang ii sportiva\n","ranchero\n","special\n","model k\n","customline\n","crestline\n","verona\n","zx2\n","f-250 super chief\n","fx-atmos\n","prefect\n","p68\n","mondeo\n","model t\n","fiesta\n","edge\n","deluxe\n","c100\n","econoline\n","excursion\n","raider\n","f-150\n","iosis max\n","maverick\n","ikon\n","orion\n","freestyle\n","kuga\n","country squire\n","fairlane\n","crestliner\n","crown victoria\n","model y\n","scorpio\n","futura\n","freestar\n","royale\n","custom deluxe\n","mainline coupe utility\n","quadricycle\n","tourneo\n","probe ii\n","forty-nine\n","model 4-46\n","delivery car\n","bronco ii\n","focus rs wrc\n","mustang probe\n","comète\n","capri rs\n","windstar\n","falcon\n","model c ten\n","tourneo connect\n","fiesta rs wrc\n","bronco\n","carousel\n","vega\n","territory\n","vedette\n","fiera\n","indigo\n","verve\n","focus c-max\n","versailles\n","zodiac\n","cougar\n","mustang mach-e\n","model 91\n","taurus x\n","model 82a\n","4-trac\n","gt40\n","th!nk\n","standard\n","skyliner\n","vendome\n","interceptor\n","explorer america\n","fiesta st\n","probe i\n","airstream\n","ka\n","model a\n","parklane\n","atlas\n","mustang gtp\n","transit custom\n","eifel\n","abeille\n","rheinland\n","corcel\n","pronto\n","transit\n","rs200\n","model 7377\n","victoria\n","spectron\n","focus ma\n","300\n","aerostar\n","pockar\n","shelby gr-1\n","synus\n","laser\n","s-max\n","iosis\n","model 21a2ga\n","model 69a6ga\n","everest\n","exp\n","model s\n","hfx aerostar\n","mustang\n","expedition\n","executive\n","model 18\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DXC8lReBW0pg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1597774169272,"user_tz":-330,"elapsed":2166,"user":{"displayName":"Akash Ravichandran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhfKFX11vGh6cZxd-XFGzg8KZ0eqt7nBXu2mgPT1Q=s64","userId":"05259929966764095227"}},"outputId":"3cb84ac4-02e7-4b15-9a77-eaf61e5e131b"},"source":["char_to_ix = { ch:i for i,ch in enumerate(sorted(chars)) }\n","ix_to_char = { i:ch for i,ch in enumerate(sorted(chars)) }\n","print(ix_to_char)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{0: '\\n', 1: ' ', 2: '!', 3: '&', 4: '-', 5: '.', 6: '0', 7: '1', 8: '2', 9: '3', 10: '4', 11: '5', 12: '6', 13: '7', 14: '8', 15: '9', 16: 'a', 17: 'b', 18: 'c', 19: 'd', 20: 'e', 21: 'f', 22: 'g', 23: 'h', 24: 'i', 25: 'k', 26: 'l', 27: 'm', 28: 'n', 29: 'o', 30: 'p', 31: 'q', 32: 'r', 33: 's', 34: 't', 35: 'u', 36: 'v', 37: 'w', 38: 'x', 39: 'y', 40: 'z', 41: 'è', 42: 'ö'}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PC4TYxf3W44A","colab_type":"code","colab":{}},"source":["### GRADED FUNCTION: clip\n","\n","def clip(gradients, maxValue):\n","    '''\n","    Clips the gradients' values between minimum and maximum.\n","    \n","    Arguments:\n","    gradients -- a dictionary containing the gradients \"dWaa\", \"dWax\", \"dWya\", \"db\", \"dby\"\n","    maxValue -- everything above this number is set to this number, and everything less than -maxValue is set to -maxValue\n","    \n","    Returns: \n","    gradients -- a dictionary with the clipped gradients.\n","    '''\n","    \n","    dWaa, dWax, dWya, db, dby = gradients['dWaa'], gradients['dWax'], gradients['dWya'], gradients['db'], gradients['dby']\n","   \n","    ### START CODE HERE ###\n","    # clip to mitigate exploding gradients, loop over [dWax, dWaa, dWya, db, dby]. (≈2 lines)\n","    for gradient in [dWax, dWaa, dWya, db, dby]:\n","        np.clip(gradient, -maxValue, maxValue, out=gradient)\n","    ### END CODE HERE ###\n","    \n","    gradients = {\"dWaa\": dWaa, \"dWax\": dWax, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n","    \n","    return gradients"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5Dc6bI2kXDVY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":102},"executionInfo":{"status":"ok","timestamp":1597774169273,"user_tz":-330,"elapsed":2149,"user":{"displayName":"Akash Ravichandran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhfKFX11vGh6cZxd-XFGzg8KZ0eqt7nBXu2mgPT1Q=s64","userId":"05259929966764095227"}},"outputId":"8aa4e07b-ac41-4429-867c-359351d7c98b"},"source":["np.random.seed(3)\n","dWax = np.random.randn(5,3)*10\n","dWaa = np.random.randn(5,5)*10\n","dWya = np.random.randn(2,5)*10\n","db = np.random.randn(5,1)*10\n","dby = np.random.randn(2,1)*10\n","gradients = {\"dWax\": dWax, \"dWaa\": dWaa, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n","gradients = clip(gradients, 10)\n","print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients[\"dWaa\"][1][2])\n","print(\"gradients[\\\"dWax\\\"][3][1] =\", gradients[\"dWax\"][3][1])\n","print(\"gradients[\\\"dWya\\\"][1][2] =\", gradients[\"dWya\"][1][2])\n","print(\"gradients[\\\"db\\\"][4] =\", gradients[\"db\"][4])\n","print(\"gradients[\\\"dby\\\"][1] =\", gradients[\"dby\"][1])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["gradients[\"dWaa\"][1][2] = 10.0\n","gradients[\"dWax\"][3][1] = -10.0\n","gradients[\"dWya\"][1][2] = 0.2971381536101662\n","gradients[\"db\"][4] = [10.]\n","gradients[\"dby\"][1] = [8.45833407]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lH9QiV0pXFwA","colab_type":"code","colab":{}},"source":["# GRADED FUNCTION: sample\n","\n","def sample(parameters, char_to_ix, seed):\n","    \"\"\"\n","    Sample a sequence of characters according to a sequence of probability distributions output of the RNN\n","\n","    Arguments:\n","    parameters -- python dictionary containing the parameters Waa, Wax, Wya, by, and b. \n","    char_to_ix -- python dictionary mapping each character to an index.\n","    seed -- used for grading purposes. Do not worry about it.\n","\n","    Returns:\n","    indices -- a list of length n containing the indices of the sampled characters.\n","    \"\"\"\n","    \n","    # Retrieve parameters and relevant shapes from \"parameters\" dictionary\n","    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n","    vocab_size = by.shape[0]\n","    n_a = Waa.shape[1]\n","    \n","    ### START CODE HERE ###\n","    # Step 1: Create the one-hot vector x for the first character (initializing the sequence generation). (≈1 line)\n","    x = np.zeros((vocab_size, 1))\n","    # Step 1': Initialize a_prev as zeros (≈1 line)\n","    a_prev = np.zeros((n_a, 1))\n","    \n","    # Create an empty list of indices, this is the list which will contain the list of indices of the characters to generate (≈1 line)\n","    indices = []\n","    \n","    # Idx is a flag to detect a newline character, we initialize it to -1\n","    idx = -1 \n","    \n","    # Loop over time-steps t. At each time-step, sample a character from a probability distribution and append \n","    # its index to \"indices\". We'll stop if we reach 50 characters (which should be very unlikely with a well \n","    # trained model), which helps debugging and prevents entering an infinite loop. \n","    counter = 0\n","    newline_character = char_to_ix['\\n']\n","    \n","    while (idx != newline_character and counter != 50):\n","        \n","        # Step 2: Forward propagate x using the equations (1), (2) and (3)\n","        a = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + b)\n","        z = np.dot(Wya, a) + by\n","        y = softmax(z)\n","        \n","        # for grading purposes\n","        np.random.seed(counter + seed) \n","        \n","        # Step 3: Sample the index of a character within the vocabulary from the probability distribution y\n","        idx = np.random.choice(list(range(vocab_size)), p=y.ravel())\n","\n","        # Append the index to \"indices\"\n","        indices.append(idx)\n","        \n","        # Step 4: Overwrite the input character as the one corresponding to the sampled index.\n","        x = np.zeros((vocab_size, 1))\n","        x[idx] = 1\n","        \n","        # Update \"a_prev\" to be \"a\"\n","        a_prev = a\n","        \n","        # for grading purposes\n","        seed += 1\n","        counter +=1\n","        \n","    ### END CODE HERE ###\n","\n","    if (counter == 50):\n","        indices.append(char_to_ix['\\n'])\n","    \n","    return indices"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"urtCc0RIcNtZ","colab_type":"code","colab":{}},"source":["import numpy as np\n","\n","def softmax(x):\n","    e_x = np.exp(x - np.max(x))\n","    return e_x / e_x.sum(axis=0)\n","\n","def smooth(loss, cur_loss):\n","    return loss * 0.999 + cur_loss * 0.001\n","\n","def print_sample(sample_ix, ix_to_char):\n","    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n","    txt = txt[0].upper() + txt[1:]  # capitalize first character \n","    print ('%s' % (txt, ), end='')\n","\n","def get_initial_loss(vocab_size, seq_length):\n","    return -np.log(1.0/vocab_size)*seq_length\n","\n","def softmax(x):\n","    e_x = np.exp(x - np.max(x))\n","    return e_x / e_x.sum(axis=0)\n","\n","def initialize_parameters(n_a, n_x, n_y):\n","    \"\"\"\n","    Initialize parameters with small random values\n","    \n","    Returns:\n","    parameters -- python dictionary containing:\n","                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n","                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n","                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n","                        b --  Bias, numpy array of shape (n_a, 1)\n","                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n","    \"\"\"\n","    np.random.seed(1)\n","    Wax = np.random.randn(n_a, n_x)*0.01 # input to hidden\n","    Waa = np.random.randn(n_a, n_a)*0.01 # hidden to hidden\n","    Wya = np.random.randn(n_y, n_a)*0.01 # hidden to output\n","    b = np.zeros((n_a, 1)) # hidden bias\n","    by = np.zeros((n_y, 1)) # output bias\n","    \n","    parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b,\"by\": by}\n","    \n","    return parameters\n","\n","def rnn_step_forward(parameters, a_prev, x):\n","    \n","    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n","    a_next = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + b) # hidden state\n","    p_t = softmax(np.dot(Wya, a_next) + by) # unnormalized log probabilities for next chars # probabilities for next chars \n","    \n","    return a_next, p_t\n","\n","def rnn_step_backward(dy, gradients, parameters, x, a, a_prev):\n","    \n","    gradients['dWya'] += np.dot(dy, a.T)\n","    gradients['dby'] += dy\n","    da = np.dot(parameters['Wya'].T, dy) + gradients['da_next'] # backprop into h\n","    daraw = (1 - a * a) * da # backprop through tanh nonlinearity\n","    gradients['db'] += daraw\n","    gradients['dWax'] += np.dot(daraw, x.T)\n","    gradients['dWaa'] += np.dot(daraw, a_prev.T)\n","    gradients['da_next'] = np.dot(parameters['Waa'].T, daraw)\n","    return gradients\n","\n","def update_parameters(parameters, gradients, lr):\n","\n","    parameters['Wax'] += -lr * gradients['dWax']\n","    parameters['Waa'] += -lr * gradients['dWaa']\n","    parameters['Wya'] += -lr * gradients['dWya']\n","    parameters['b']  += -lr * gradients['db']\n","    parameters['by']  += -lr * gradients['dby']\n","    return parameters\n","\n","def rnn_forward(X, Y, a0, parameters, vocab_size = 43):\n","    \n","    # Initialize x, a and y_hat as empty dictionaries\n","    x, a, y_hat = {}, {}, {}\n","    \n","    a[-1] = np.copy(a0)\n","    \n","    # initialize your loss to 0\n","    loss = 0\n","    \n","    for t in range(len(X)):\n","        \n","        # Set x[t] to be the one-hot vector representation of the t'th character in X.\n","        # if X[t] == None, we just have x[t]=0. This is used to set the input for the first timestep to the zero vector. \n","        x[t] = np.zeros((vocab_size,1)) \n","        if (X[t] != None):\n","            x[t][X[t]] = 1\n","        \n","        # Run one step forward of the RNN\n","        a[t], y_hat[t] = rnn_step_forward(parameters, a[t-1], x[t])\n","        \n","        # Update the loss by substracting the cross-entropy term of this time-step from it.\n","        loss -= np.log(y_hat[t][Y[t],0])\n","        \n","    cache = (y_hat, a, x)\n","        \n","    return loss, cache\n","\n","def rnn_backward(X, Y, parameters, cache):\n","    # Initialize gradients as an empty dictionary\n","    gradients = {}\n","    \n","    # Retrieve from cache and parameters\n","    (y_hat, a, x) = cache\n","    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n","    \n","    # each one should be initialized to zeros of the same dimension as its corresponding parameter\n","    gradients['dWax'], gradients['dWaa'], gradients['dWya'] = np.zeros_like(Wax), np.zeros_like(Waa), np.zeros_like(Wya)\n","    gradients['db'], gradients['dby'] = np.zeros_like(b), np.zeros_like(by)\n","    gradients['da_next'] = np.zeros_like(a[0])\n","    \n","    ### START CODE HERE ###\n","    # Backpropagate through time\n","    for t in reversed(range(len(X))):\n","        dy = np.copy(y_hat[t])\n","        dy[Y[t]] -= 1\n","        gradients = rnn_step_backward(dy, gradients, parameters, x[t], a[t], a[t-1])\n","    ### END CODE HERE ###\n","    \n","    return gradients, a"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X7SOrB62XOwY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1597774169744,"user_tz":-330,"elapsed":2600,"user":{"displayName":"Akash Ravichandran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhfKFX11vGh6cZxd-XFGzg8KZ0eqt7nBXu2mgPT1Q=s64","userId":"05259929966764095227"}},"outputId":"c3227153-0085-4440-f98e-3b125d40bb79"},"source":["np.random.seed(2)\n","_, n_a = 20, 100\n","Wax, Waa, Wya = np.random.randn(n_a, vocab_size), np.random.randn(n_a, n_a), np.random.randn(vocab_size, n_a)\n","b, by = np.random.randn(n_a, 1), np.random.randn(vocab_size, 1)\n","parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b, \"by\": by}\n","\n","indices = sample(parameters, char_to_ix, 0)\n","print(\"Sampling:\")\n","print(\"list of sampled indices:\", indices)\n","print(\"list of sampled characters:\", [ix_to_char[i] for i in indices])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Sampling:\n","list of sampled indices: [18, 11, 11, 25, 42, 24, 35, 26, 3, 26, 20, 7, 20, 10, 0]\n","list of sampled characters: ['c', '5', '5', 'k', 'ö', 'i', 'u', 'l', '&', 'l', 'e', '1', 'e', '4', '\\n']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2EVdP9w_XS-w","colab_type":"code","colab":{}},"source":["# GRADED FUNCTION: optimize\n","\n","def optimize(X, Y, a_prev, parameters, learning_rate = 0.01):\n","    \"\"\"\n","    Execute one step of the optimization to train the model.\n","    \n","    Arguments:\n","    X -- list of integers, where each integer is a number that maps to a character in the vocabulary.\n","    Y -- list of integers, exactly the same as X but shifted one index to the left.\n","    a_prev -- previous hidden state.\n","    parameters -- python dictionary containing:\n","                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n","                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n","                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n","                        b --  Bias, numpy array of shape (n_a, 1)\n","                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n","    learning_rate -- learning rate for the model.\n","    \n","    Returns:\n","    loss -- value of the loss function (cross-entropy)\n","    gradients -- python dictionary containing:\n","                        dWax -- Gradients of input-to-hidden weights, of shape (n_a, n_x)\n","                        dWaa -- Gradients of hidden-to-hidden weights, of shape (n_a, n_a)\n","                        dWya -- Gradients of hidden-to-output weights, of shape (n_y, n_a)\n","                        db -- Gradients of bias vector, of shape (n_a, 1)\n","                        dby -- Gradients of output bias vector, of shape (n_y, 1)\n","    a[len(X)-1] -- the last hidden state, of shape (n_a, 1)\n","    \"\"\"\n","    \n","    ### START CODE HERE ###\n","    \n","    # Forward propagate through time (≈1 line)\n","    loss, cache = rnn_forward(X, Y, a_prev, parameters)\n","    \n","    # Backpropagate through time (≈1 line)\n","    gradients, a = rnn_backward(X, Y, parameters, cache)\n","    \n","    # Clip your gradients between -5 (min) and 5 (max) (≈1 line)\n","    gradients = clip(gradients, 5)\n","    \n","    # Update parameters (≈1 line)\n","    parameters = update_parameters(parameters, gradients, learning_rate)\n","    \n","    ### END CODE HERE ###\n","    \n","    return loss, gradients, a[len(X)-1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HWWgvi3ZcZVV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"executionInfo":{"status":"ok","timestamp":1597774169748,"user_tz":-330,"elapsed":2589,"user":{"displayName":"Akash Ravichandran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhfKFX11vGh6cZxd-XFGzg8KZ0eqt7nBXu2mgPT1Q=s64","userId":"05259929966764095227"}},"outputId":"e5e68cac-86a9-45d7-877c-986ebb36b414"},"source":["np.random.seed(1)\n","vocab_size, n_a = 43, 100\n","a_prev = np.random.randn(n_a, 1)\n","Wax, Waa, Wya = np.random.randn(n_a, vocab_size), np.random.randn(n_a, n_a), np.random.randn(vocab_size, n_a)\n","b, by = np.random.randn(n_a, 1), np.random.randn(vocab_size, 1)\n","parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b, \"by\": by}\n","X = [12,3,5,11,22,3]\n","Y = [4,14,11,22,25, 26]\n","\n","loss, gradients, a_last = optimize(X, Y, a_prev, parameters, learning_rate = 0.01)\n","print(\"Loss =\", loss)\n","print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients[\"dWaa\"][1][2])\n","print(\"np.argmax(gradients[\\\"dWax\\\"]) =\", np.argmax(gradients[\"dWax\"]))\n","print(\"gradients[\\\"dWya\\\"][1][2] =\", gradients[\"dWya\"][1][2])\n","print(\"gradients[\\\"db\\\"][4] =\", gradients[\"db\"][4])\n","print(\"gradients[\\\"dby\\\"][1] =\", gradients[\"dby\"][1])\n","print(\"a_last[4] =\", a_last[4])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loss = 122.39761350325455\n","gradients[\"dWaa\"][1][2] = 5.0\n","np.argmax(gradients[\"dWax\"]) = 46\n","gradients[\"dWya\"][1][2] = -3.3343495199573015e-06\n","gradients[\"db\"][4] = [0.42338138]\n","gradients[\"dby\"][1] = [3.39311587e-06]\n","a_last[4] = [0.57299753]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"D9hTNN1tce1Y","colab_type":"code","colab":{}},"source":["# GRADED FUNCTION: model\n","\n","def model(data, ix_to_char, char_to_ix, num_iterations = 100000, n_a = 50, dino_names = 7, vocab_size = 43):\n","    \"\"\"\n","    Trains the model and generates dinosaur names. \n","    \n","    Arguments:\n","    data -- text corpus\n","    ix_to_char -- dictionary that maps the index to a character\n","    char_to_ix -- dictionary that maps a character to an index\n","    num_iterations -- number of iterations to train the model for\n","    n_a -- number of units of the RNN cell\n","    dino_names -- number of dinosaur names you want to sample at each iteration. \n","    vocab_size -- number of unique characters found in the text, size of the vocabulary\n","    \n","    Returns:\n","    parameters -- learned parameters\n","    \"\"\"\n","    \n","    # Retrieve n_x and n_y from vocab_size\n","    n_x, n_y = vocab_size, vocab_size\n","    \n","    # Initialize parameters\n","    parameters = initialize_parameters(n_a, n_x, n_y)\n","    \n","    # Initialize loss (this is required because we want to smooth our loss, don't worry about it)\n","    loss = get_initial_loss(vocab_size, dino_names)\n","    \n","    # Build list of all dinosaur names (training examples).\n","    with open(\"output.txt\") as f:\n","        examples = f.readlines()\n","    examples = [x.lower().strip() for x in examples]\n","    \n","    # Shuffle list of all dinosaur names\n","    np.random.seed(0)\n","    np.random.shuffle(examples)\n","    \n","    # Initialize the hidden state of your LSTM\n","    a_prev = np.zeros((n_a, 1))\n","    \n","    # Optimization loop\n","    for j in range(num_iterations):\n","        \n","        ### START CODE HERE ###\n","        \n","        # Use the hint above to define one training example (X,Y) (≈ 2 lines)\n","        index = j % len(examples)\n","        X = [None] + [char_to_ix[ch] for ch in examples[index]] \n","        Y = X[1:] + [char_to_ix[\"\\n\"]]\n","        \n","        # Perform one optimization step: Forward-prop -> Backward-prop -> Clip -> Update parameters\n","        # Choose a learning rate of 0.01\n","        curr_loss, gradients, a_prev = optimize(X, Y, a_prev, parameters)\n","        \n","        ### END CODE HERE ###\n","        \n","        # Use a latency trick to keep the loss smooth. It happens here to accelerate the training.\n","        loss = smooth(loss, curr_loss)\n","\n","        # Every 2000 Iteration, generate \"n\" characters thanks to sample() to check if the model is learning properly\n","        if j % 2000 == 0:\n","            \n","            print('Iteration: %d, Loss: %f' % (j, loss) + '\\n')\n","            \n","            # The number of dinosaur names to print\n","            seed = 0\n","            for name in range(dino_names):\n","                \n","                # Sample indices and print them\n","                sampled_indices = sample(parameters, char_to_ix, seed)\n","                print_sample(sampled_indices, ix_to_char)\n","                \n","                seed += 1  # To get the same result for grading purposed, increment the seed by one. \n","      \n","            print('\\n')\n","        \n","    return parameters"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zBmPZ2kgck-g","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1597774255035,"user_tz":-330,"elapsed":87863,"user":{"displayName":"Akash Ravichandran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhfKFX11vGh6cZxd-XFGzg8KZ0eqt7nBXu2mgPT1Q=s64","userId":"05259929966764095227"}},"outputId":"9d65c928-216f-45ba-f9f7-fd1837b3cccc"},"source":["parameters = model(data, ix_to_char, char_to_ix)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Iteration: 0, Loss: 26.324640\n","\n","Hcèxws0g3mk2è7qmv qabaus\n","Bh3&\n","Cèxws0g3mk2è7qmv qabaus\n","H3&\n","Èxws0g3mk2è7qmv qabaus\n","3&\n","Xws0g3mk2è7qmv qabaus\n","\n","\n","Iteration: 2000, Loss: 27.995832\n","\n","Kewttobfaon xarmo\n","En  \n","Ewttobfaon xarmo\n","K7\n","Xttp k8on xarmo\n","7\n","Ttr lapiaudnos\n","\n","\n","Iteration: 4000, Loss: 24.765906\n","\n","Iextrodel i.vargp0omelir\n","El 0\n","Extrodel i.vargp0omelir\n","Ia\n","Xtrocgare wanlo-madero\n","8\n","Trocgaphax2nos\n","\n","\n","Iteration: 6000, Loss: 22.411641\n","\n","Laxnor g3850westo\n","Fia\n","Guvem dele8uingo\n","La\n","Wustafcorcufrer\n","C5\n","Troci modilanh ra\n","\n","\n","Iteration: 8000, Loss: 20.560641\n","\n","Mlynosconpiey\n","Io\n","Kuvfrac\n","Ma\n","Utye\n","Ca\n","Tro\n","\n","\n","Iteration: 10000, Loss: 19.711997\n","\n","Mosthaiielikt\n","Ga\n","Ivori mare\n","Mec\n","Ttrr\n","E\n","Sur\n","\n","\n","Iteration: 12000, Loss: 19.083174\n","\n","Mostorbe\n","Ga\n","Gusto ie\n","Ma\n","Trondeialex moselcpert\n","Ca\n","Stobl re\n","\n","\n","Iteration: 14000, Loss: 18.307741\n","\n","Mostlmal\n","Ga\n","Gtor\n","Ma\n","Trustang excont raednl\n","Ca\n","Sus\n","\n","\n","Iteration: 16000, Loss: 17.816140\n","\n","Gautre\n","Gab1\n","Gustocrerokun rucur fso1erevee\n","Ioc\n","Tutra!geo\n","E6\n","Sus\n","\n","\n","Iteration: 18000, Loss: 17.698783\n","\n","Garivefielinplexi\n","Fiec\n","Fvett\n","Ga\n","Toura\n","Ca\n","Suran grastor 1ine\n","\n","\n","Iteration: 20000, Loss: 17.534306\n","\n","Gausto rerrangoculeg3modaliry\n","Fief\n","Futpromedelmgine\n","Ga\n","Tprobe ii\n","Ca\n","Ronco spex\n","\n","\n","Iteration: 22000, Loss: 16.701299\n","\n","Gestrnan\n","Gie\n","Gustom ctorilinl\n","Ga\n","Toura\n","Ca\n","Super\n","\n","\n","Iteration: 24000, Loss: 16.640858\n","\n","Fipotmai\n","Cole\n","Explone\n","Fa 150\n","Tronce\n","C-\n","Surcomodel c\n","\n","\n","Iteration: 26000, Loss: 16.369059\n","\n","Gatroper\n","Focc-grodee r\n","Ftouranconco touire\n","Ga\n","Vertam\n","Ca\n","Super\n","\n","\n","Iteration: 28000, Loss: 16.061633\n","\n","Fistor\n","Ex 0\n","Explonee counect\n","Fa\n","Turderereetgura\n","C-\n","Purce fomy\n","\n","\n","Iteration: 30000, Loss: 16.261775\n","\n","Gausus\n","Foca\n","Fustom 500\n","Ga\n","Vorseiosphuskoscorodit73sporr\n","C--axra\n","Super dicr gll\n","\n","\n","Iteration: 32000, Loss: 17.499582\n","\n","Mostrpetdepiversiastontr\n","Mode ive\n","Muttomodel v\n","Ma\n","Tusnan\n","Fa-9prn2aqrith!nusoat\n","Tur\n","\n","\n","Iteration: 34000, Loss: 16.591829\n","\n","Mostor\n","Gae\n","Lustang mact ros\n","Ma\n","Ttradee iis orpang coufiontd a20\n","C-!19ga\n","Sus faplaueroranadeer\n","\n","\n","Iteration: 36000, Loss: 16.206143\n","\n","Mosterskale\n","Mode\n","Mustang ii\n","Model r\n","Ttrr\n","F--40-model r\n","Susireis urtyl\n","\n","\n","Iteration: 38000, Loss: 15.508944\n","\n","Montourectecuston\n","Mode\n","Mustang n frango\n","Modaigom\n","Vustanaindtrint\n","F--59ta\n","Surcorth x2a1g5ac\n","\n","\n","Iteration: 40000, Loss: 15.689747\n","\n","Mostor\n","Fra1\n","Fustoma\n","Modcaiskr\n","Turk\n","Ca\n","Sus rabe umoos traest\n","\n","\n","Iteration: 42000, Loss: 15.893301\n","\n","Model 427welutrii\n","La0\n","Model 81a\n","Modcl r\n","Trorer ii\n","E7\n","Rom6d\n","\n","\n","Iteration: 44000, Loss: 16.329799\n","\n","Montry\n","Mode\n","Mustang megte\n","Modderka\n","Vmoska\n","El kel 8\n","Psura\n","\n","\n","Iteration: 46000, Loss: 15.945752\n","\n","Fiotra\n","Ex\n","Exprodel 91a0dal\n","Fiec\n","Vort\n","C-\n","Tronchen\n","\n","\n","Iteration: 48000, Loss: 16.929581\n","\n","Montor\n","Mode  rpe50er sportl 79-7er\n","Mustang raitir\n","Ma\n","Vostal\n","Fa\n","Treerori\n","\n","\n","Iteration: 50000, Loss: 18.706122\n","\n","Muttra\n","Goc\n","Lusto6\n","Model n-a\n","Vustancougumant\n","F--19n\n","Sures oret ronalanert\n","\n","\n","Iteration: 52000, Loss: 22.995195\n","\n","Mostor7007f\n","Ing\n","Kusto r me\n","Med\n","Vostendel t0inm\n","F \n","Sus r rrcteles ogecus\n","\n","\n","Iteration: 54000, Loss: 26.902728\n","\n","Iitrrranapl x1orp\n","Fi7\n","Gutoran\n","Ie\n","Vrup2l ngaxerpp\n","E \n","Trr\n","\n","\n","Iteration: 56000, Loss: 27.281776\n","\n","Movron m moduconr\n","Im 2\n","Ixstr i8modtaons\n","Mo \n","Vrss mapi wconr\n","F0\n","Sto mapi wconr\n","\n","\n","Iteration: 58000, Loss: 26.723545\n","\n","Gitttpal5mgaukipt\n","Fi2\n","Fwstr\n","Ga\n","Tptr g6opcuiros\n","E1\n","Str mconcugtos\n","\n","\n","Iteration: 60000, Loss: 27.069242\n","\n","Fexsto\n","Fie\n","Fusti danl sfont\n","Fa\n","Trsr5dani\n","F-\n","Surai8pk\n","\n","\n","Iteration: 62000, Loss: 27.831344\n","\n","Moxtto4manm wcons\n","God\n","Ixstobkane xaros\n","Mic\n","Tttpcomonaueror\n","Fa\n","Sur g6pmaufpor\n","\n","\n","Iteration: 64000, Loss: 27.593698\n","\n","Gextrobnamfaucorr\n","Foa\n","Fustoam5il vemos\n","Ga\n","Utrt8ecri warmo igbaro\n","E\n","Sus-garoctarir\n","\n","\n","Iteration: 66000, Loss: 27.358275\n","\n","Kexsttafarpaverns\n","Gef \n","Gustk-ranraucsor\n","K10\n","Tutranbincucort\n","F-\n","Tur e7poculont\n","\n","\n","Iteration: 68000, Loss: 27.079469\n","\n","G6vttrai lecudeos\n","Fga\n","Gustt miricuereo\n","G10\n","Tttobgconcydpou\n","C-\n","Pusal mofterlt-ogairt\n","\n","\n","Iteration: 70000, Loss: 26.514735\n","\n","Gaxtrocmaredueros\n","Gec \n","Gustr\n","G00\n","Tustancorcugrls\n","E\n","Rushocol xcort\n","\n","\n","Iteration: 72000, Loss: 26.564573\n","\n","Gaxtoobo mi xcorr\n","Ge9\n","Gustobcaomev per\n","G61\n","Ttsran6ig6saont\n","E\n","Pus!f\n","\n","\n","Iteration: 74000, Loss: 27.064202\n","\n","Mextoramame7vanmo\n","Gid\n","Gustobeami8vconr\n","Ma6\n","Tuspanangaterho\n","F0\n","Sus gapeavangp\n","\n","\n","Iteration: 76000, Loss: 27.068246\n","\n","Mexsur\n","Go8.\n","Gusus f6mi4u0les\n","M \n","Tust6g kdav6mlu\n","F \n","Sus6i kecucpos\n","\n","\n","Iteration: 78000, Loss: 27.650513\n","\n","Mivrtramasscumort\n","Lod\n","Mutto8mariauerir\n","Ma\n","Ttst7m7mocueros\n","F \n","Ttr marecugoos\n","\n","\n","Iteration: 80000, Loss: 28.056079\n","\n","Mextor i\n","Gec \n","Gutqp\n","Ma\n","Vtto0edni\n","C \n","Sto e-mi\n","\n","\n","Iteration: 82000, Loss: 28.018582\n","\n","Lexttr marqavarlt\n","Gfa\n","Gustobe7ii6varmo\n","L -\n","Ustobefon-udios\n","A\n","Tstar plataror\n","\n","\n","Iteration: 84000, Loss: 28.486349\n","\n","Mowttocgaro2uaros\n","Moa\n","Muttr i oraucort\n","Ma\n","Uutp m2nmavdpos\n","F \n","Tur laolaucros\n","\n","\n","Iteration: 86000, Loss: 28.796864\n","\n","Mextss\n","Hmb\n","Ivtss0gane7xconr\n","Mb \n","Usto-f3rf6xcoot\n","E\n","Tss\n","\n","\n","Iteration: 88000, Loss: 28.975222\n","\n","Mextsr\n","Gmc \n","Guttp\n","M 0\n","Vusraickmaxdpmr\n","D\n","Ttran\n","\n","\n","Iteration: 90000, Loss: 28.915510\n","\n","Kivstp l nl9udons\n","Fn \n","Fusst icnn ueons\n","K2 \n","Uuso i lnaueons\n","C\n","Sup5f miaucons\n","\n","\n","Iteration: 92000, Loss: 27.789181\n","\n","Ievttp\n","Foc\n","Fustp ldom7udpos\n","Ic\n","Ustoaedimcudopt\n","C-\n","Tur0iconduepkt\n","\n","\n","Iteration: 94000, Loss: 27.176054\n","\n","Gnvtrodicoocudoos\n","Eo4\n","Futtodiconcudoos\n","Gh\n","Vtso\n","C-\n","Sus\n","\n","\n","Iteration: 96000, Loss: 26.541955\n","\n","Fnxtro gaoodtknmr\n","Ere\n","Extor f modoliis\n","Fa\n","Uspo g7modoliis\n","E\n","Sum-i modiliis\n","\n","\n","Iteration: 98000, Loss: 27.154411\n","\n","Mottso n\n","Fod\n","Gyssobecoo1ueros\n","Ml\n","Tusodellodtaros\n","E\n","Sus\n","\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DaXUrdCvcq4x","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}